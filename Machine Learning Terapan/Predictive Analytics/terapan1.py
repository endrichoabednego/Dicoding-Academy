# -*- coding: utf-8 -*-
"""Terapan1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f4PzAqTRfsXVlHrli4kBApOYdNAO3Rg1

## **Import Library**
Melakukan import library yang diperlukan untuk membuat model
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import zipfile,os, random, cv2, PIL, pathlib
from tensorflow.python import metrics
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16

"""## **Dataset Preparation**
Memanggil dataset yang ada dalam kaggle. Saya menggunakan dataset Shoe-vs-sandal-vs-boot-dataset-15k-images.
https://www.kaggle.com/datasets/hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images

Dataset diatas berisi 3 varian alas kaki yang dibagi menjadi 3 folder. Setiap varian berisi 5000 data gambar yang akan digunakan untuk train model dengan total data 15000 gambar. Varian dari dataset ini sesuai judulnya adalah Shoe, Sandal, dan Boot.  
"""

! pip install kaggle
! mkdir ~/.kaggle 
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download muratkokludataset/pistachio-image-dataset

! unzip pistachio-image-dataset.zip

"""## **Split Dataset**
Dataset dibagi dalam 3 rasio : 
<br>train dataset : 80% = 12000 data
<br>validation dataset : 10% = 1500 data
<br>test dataset : 10% = 1500 data
"""

! pip install split-folders
import splitfolders

basedirect = "Pistachio_Image_Dataset/Pistachio_Image_Dataset"
base_ds = pathlib.Path(basedirect)

splitfolders.ratio(
    basedirect, 
    output = "imgs", 
    seed = 123, 
    ratio = (.8,.2), 
    group_prefix = None
)

"""## Image Labelling
Pada bagian ini digunakan untuk proses pemberian label pada dataset
"""

pistachio_label = ["Kirmizi_Pistachio","Siirt_Pistachio"]

kirmizi = [fn for fn in os.listdir(f'{basedirect}/Kirmizi_Pistachio') if fn.endswith('.jpg')]
siirt = [fn for fn in os.listdir(f'{basedirect}/Siirt_Pistachio') if fn.endswith('.jpg')]
pistachio = [kirmizi,siirt]

"""## Dataset Visualitation
Selanjutnya diperlukan untuk melakukan visualisasi data supaya dapat lebih memahami dataset yang akan digunakan
"""

for i in range (4):
  plt.figure(figsize=(10,10))
  for i in range(len(pistachio_label)):
    plt.subplot(1, 5, i+1)
    file = random.choice(os.listdir(basedirect + "/" + pistachio_label[i]))
    image_path = os.path.join(basedirect + "/" + pistachio_label[i], file)
    img = cv2.imread(image_path)
    plt.title(pistachio_label[i])
    plt.imshow(img)
    plt.axis("off")
    plt.grid(None)

train_dir = os.path.join(basedirect,'imgs/train')
val_dir = os.path.join(basedirect,'imgs/val')

"""##Augmentasi Gambar 
Pada proses ini akan dilakukan augmentasi gambar yang memiliki fungsi untuk meningkatkan ukuran kumpulan data gambar secara artifiisial hal ini dapat dicapai dengan menerapkan transformasi aca ke dalam gambar.
"""

train_datagen = ImageDataGenerator(
      rescale = 1.0/255,
      horizontal_flip = True,
      shear_range = 0.2,
      zoom_range = 0.2, 
      rotation_range = 20,
      width_shift_range = 0.2,
      height_shift_range = 0.2,
      vertical_flip = False,
      fill_mode = 'nearest',
      validation_split = 0.2)

validation_datagen = ImageDataGenerator(
    rescale = 1.0/255


)

"""## FLow Train Set Data dan Validation Set Data

Proses load data ke dalam memori 
"""

train_set = train_datagen.flow_from_directory(
    'imgs/train',
    target_size = (175, 175),
    batch_size = 32,
    class_mode='categorical'
)

val_set = validation_datagen.flow_from_directory(
    'imgs/val',
    target_size = (175, 175),
    batch_size = 32,
    class_mode='categorical',
)

"""## Modelling
Proyek ini menggunakan metode Conv2D Maxpool LayerConv2D Layer untuk melakukan permodelan
"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32,(3,3),activation= 'relu',input_shape=(175,175,3)),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(64,(3,3),activation= 'relu'),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(128,(3,3),activation= 'relu'),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(256,(3,3),activation= 'relu'),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512,activation= 'relu'),
    tf.keras.layers.Dense(2,activation= 'softmax')
])
model.summary()

"""## Compile Model
Setelah layer telah terbangun, maka akan dicompile menggunakan optimizer adam dengan loss categorical_crossentropy dan menggunakan metrik accuracy
"""

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

"""## Callback
Pada bagian ini akan digunakan fungsi callback yang berfungsi untuk menghentikan proses pelatihan data ketika sudah mencapai kondisi yang telah ditentukan
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.98 and logs.get('val_accuracy')>0.98):
      print("\nAkurasi sudah diatas 98%, Bagus!")
      self.model.stop_training = True

callbacks = myCallback()

"""## Proses Training Model
Setelah melakukan beberapa proses diatas, maka akan dilakukan training model dengan memanggil fungsi fit. Serta pada training model ini, akan digunakan juga fungsi callback yang telah dibangun sebelumnya
"""

history = model.fit(train_set, 
                    epochs = 55, 
                    steps_per_epoch = 50,
                    validation_data = val_set, 
                    validation_steps = 10,
                    verbose = 2,
                    callbacks = callbacks)

"""## Grafik Akurasi 
Berikut merupakan grafik akurasi hasil dari training base model
"""

plt.style.use('ggplot')
plt.title("Grafik Akurasi Base Model")
plt.plot(history.history['accuracy'], label = 'Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""## Grafik Plot Loss
Berikut merupakan grafik loss hasil dari training base model
"""

plt.style.use('ggplot')
plt.title("Grafik Plot Loss Base Model")
plt.plot(history.history['loss'], label = 'Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""## Implementasi VGG16"""

vgg16 = VGG16(weights="imagenet", include_top=False, input_shape=(175, 175, 3))
vgg16.trainable = False
inputs = tf.keras.Input((175, 175, 3))
x = vgg16(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(2, activation='softmax')(x)
model_vgg16 = tf.keras.Model(inputs, x)

model_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_vgg16.summary()

checkpointer = ModelCheckpoint(
    filepath='model/model_vgg16.hdf5', 
    monitor='val_accuracy', 
    mode='max',
    verbose=2, 
    save_best_only=True
)

callbacks=[checkpointer]

history2 = model_vgg16.fit(
    train_set, 
    epochs = 20,
    validation_data = val_set, 
    callbacks = callbacks
)

"""## Grafik Akurasi VGG16
Berikut merupakan grafik akurasi hasil dari training VGG16
"""

plt.style.use('ggplot')
plt.title("Grafik Akurasi Model VGG16")
plt.plot(history2.history['accuracy'], label = 'Accuracy')
plt.plot(history2.history['val_accuracy'], label='Val Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""## Grafik Plot Loss
Berikut merupakan grafik loss hasil dari training VGG16
"""

plt.style.use('ggplot')
plt.title("Grafik Plot Loss Model VGG16")
plt.plot(history2.history['loss'], label = 'Loss')
plt.plot(history2.history['val_loss'], label='Val Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()