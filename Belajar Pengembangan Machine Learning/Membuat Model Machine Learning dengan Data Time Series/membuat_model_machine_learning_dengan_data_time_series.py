# -*- coding: utf-8 -*-
"""Membuat Model Machine Learning dengan Data Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibmufcgKS_MLabWGy008u8cq6GGis8yz

Submission Proyek Akhir : Membuat Model Machine Learning dengan Data Time Series

---


Modul : Belajar Pengembangan Machine Learning

---


Nama / ID Dicoding : Endricho Abednego / M239X0468
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Dense, LSTM
import tensorflow as tf
from google.colab import files

dataset = files.upload()

df = pd.read_csv('wind_dataset.csv')
df.head()
print(len(df))
df = df.drop(columns=['IND.1','T.MAX','IND.2','T.MIN','T.MIN.G'])

df.head()

df.isnull().sum()

date = df['DATE'].values
wind = df['WIND'].values

plt.figure(figsize=(15,5))
plt.plot(date,wind)
plt.title('Wind Average',fontsize=20);

from sklearn.model_selection import train_test_split
date_train, date_val, wind_train, wind_val = train_test_split(date, wind, test_size=0.2, shuffle=False)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1) 
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

df.head()

train_set = windowed_dataset(wind_train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(wind_val, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True,  input_shape=[None, 1]),
  tf.keras.layers.LSTM(30),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(1),
])

thresh_Mae = (df['WIND'].max() - df['WIND'].min()*10/100)
print(thresh_Mae)

optimizer = tf.keras.optimizers.SGD(learning_rate=1.000e-04, momentum=0.9)
model.compile(
    loss=tf.keras.losses.Huber(),
    optimizer=optimizer,
    metrics=['mae']
    )
history = model.fit(
    train_set,
    validation_data=val_set,
    epochs=100,
    verbose=2,
    )

train_mae = history.history['mae']
valid_mae = history.history['val_mae']
train_loss = history.history['loss']
valid_loss = history.history['val_loss']

epochs = range(1, len(train_mae) + 1)

plt.plot(epochs, train_mae, 'b',color='red', label='Training MAE')
plt.plot(epochs, valid_mae, 'b', label='Validation MAE')
plt.title('Training & Validation MAE')
plt.legend()
plt.figure()
plt.show()

plt.plot(epochs, train_loss, 'b',color='red', label='Training loss')
plt.plot(epochs, valid_loss, 'b', label='Validation loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.figure()
plt.show()