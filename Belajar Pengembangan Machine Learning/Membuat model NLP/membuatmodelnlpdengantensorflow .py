# -*- coding: utf-8 -*-
"""MembuatModelNLPdenganTensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVtoN9MsiW4EHRJ1luv8UgFACGjP2ag8

Submission Proyek Akhir : Membuat Model NLP dengan Tensorflow

---


Modul : Pengembangan Machine Learning

---


Nama / ID Dicoding : Endricho Abednego / M239X0468
"""

import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.python import metrics
from keras.models import Sequential
from google.colab import files
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense, Dropout

#melakukan upload files, karena file berada dalam local drive
dataset = files.upload()

#melakukan upload files, karena file berada dalam local drive
dataset = files.upload()

df_test = pd.read_csv('Corona_NLP_test.csv')
print(len(df_test))

#menghapus kolom yang tidak diperlukan untuk membuat suatu model 
df_test = df_test.drop(columns=['UserName','ScreenName','Location','TweetAt'])

#menampilkan dataframe yang baru setelah menghapus suatu kolom
# df

print(100/20*3798)

df_train = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')
print(len(df_train))

df_train = df_train.drop(columns=['UserName','ScreenName','Location','TweetAt'])

df_train

df_test

sentiment = pd.get_dummies(df_train.Sentiment)
df_new = pd.concat([df_train, sentiment], axis=1)
df_new = df_new.drop(columns='Sentiment')
df_new

tweet = df_new['OriginalTweet'].values
label = df_new[['Extremely Negative','Extremely Positive','Negative','Neutral','Positive']].values

tweet_train, tweet_val, label_train, label_val = train_test_split (tweet, label, test_size = 0.2)

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(tweet_train) 
tokenizer.fit_on_texts(tweet_val)
 
sequent_train = tokenizer.texts_to_sequences(tweet_train)
sequent_val = tokenizer.texts_to_sequences(tweet_val)
 
pad_train = pad_sequences(sequent_train) 
pad_val = pad_sequences(sequent_val)

model = Sequential([
    Embedding(input_dim=10000, output_dim=128),
    LSTM(64),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(
    pad_train, 
    label_train,
    epochs = 20,
    validation_data = (pad_val, label_val),
    callbacks = [
        tf.keras.callbacks.LearningRateScheduler(
            lambda epoch: 1e-3 * 10 ** (epoch / 30)
        )
    ],
    verbose=2

)

plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim(ymin=0)
plt.show()

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='validation_loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.ylim(ymin=0)
plt.show()